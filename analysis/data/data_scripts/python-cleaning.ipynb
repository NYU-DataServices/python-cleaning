{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Management Using Python\n",
    "\n",
    "**Nicholas Wolf and Vicky Steeves, NYU Data Services**\n",
    "\n",
    "Vicky's ORCID: 0000-0003-4298-168X | Nick's ORCID: 0000-0001-5512-6151\n",
    "\n",
    "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.\n",
    "\n",
    "**Overview**\n",
    "\n",
    "This session is an intermediate level class that will examine ways to perform data cleaning, transformation, and management using Python. We will look at some helpful ways to load data and parse it into a container for ease of use in Python, to store it in helpful formats, and to perform some basic cleaning and transformations typical for mixed string-and-numeric formats. Finally, we'll try putting it all together using a dataset form the NYC Open Data portal.\n",
    "\n",
    "**Setup**\n",
    "\n",
    "Let's think about helpful project organizational structures from the start. We can borrow a helpful folder structure used by Ben Marwick in his <a href=\"https://github.com/benmarwick/rrtools\">rrtools</a> R package. There are iterations of this idea in a lot of places, but the basic idea is to separate your starting data from your transformed/cleaned data, and all of this input data from your subsequent analysis, documentation, and outputs. We'll modify his tree directory slightly:\n",
    "\n",
    "<pre>analysis/\n",
    "|\n",
    "├── paper/              # Working paper for publication\n",
    "│\n",
    "│\n",
    "├── figures/            # location of the figures produced by your analysis\n",
    "|\n",
    "├── data/\n",
    "|   ├── data_scripts/   # scripts used to transform and clean raw data\n",
    "│   ├── raw_data/       # data obtained by you or from elsewhere\n",
    "|   └── cleaned_data/   # migrated format versions of raw data, with syntax corrections and integrity checks \n",
    "|\n",
    "└── docs                # documentation files for methods, parameters, data dictionaries, etc.</pre>\n",
    "\n",
    "\n",
    "Download the zipped folder package available at <a href=\"https://goo.gl/SJUrcJ\">goo.gl/SJUrcJ</a> and place it on your desktop. Uncompress the files. We'll be working with the Jupyter Notebook located at /analysis/data/data_scripts/python-cleaning.ipynb. Alternately, you can clone the course materials using \n",
    "\n",
    "<code>git clone https\\://github\\.com/NYU-DataServices\\/python-cleaning\\.git</code>\n",
    "\n",
    "You can also find this notebook (in non-executable form) at <a href=\"https://nyu-dataservices.github.io/python-cleaning/\">https://nyu-dataservices.github.io/python-cleaning/</a>\n",
    "\n",
    "You are welcome to deploy this Jupyter Notebook in the Jupyter environment of your choice (departmental hub, local instance). You can also use it on our JupyterHub instance here: https://rdm.apps.brc.stern.nyu.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Parsing\n",
    "\n",
    "**1. CSV and Dataframes**\n",
    "\n",
    "Not surprisingly, a very common and helpful way to store data is in CSV format. Most applications have some way of working with delimited files, and while not 100% standardized, CSVs provide a stable means of sharing and storing dat of all types.\n",
    "\n",
    "Python offers a <a href=\"https://docs.python.org/3/library/csv.html\">CSV module</a>, but it can be a wordy way to input and output data into your workflow. Since analysis is also often eased by using a more complex data container such as a dataframe, let's rely on the Pandas module to do the heavy lifting of importing (and later exporting) CSVs.\n",
    "\n",
    "Here, we load up a CSV storing NYC <a href=\"https://data.cityofnewyork.us/Environment/Water-Consumption-In-The-New-York-City/ia2d-e54m\">water consumption data</a> from the NYC Open Data portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "## Set a working directory for our /data parent directory\n",
    "## Mac\n",
    "directory = r'../'\n",
    "## Windows\n",
    "#directory = r'..\\data\\\\'\n",
    "\n",
    "# Load a CSV into a data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. JSON**\n",
    "\n",
    "Python's json module is compact and easy to use. Once loaded, the JSON object is treated as a full-fledged JSON dictionary. Let's load the same data on water consumption stored as a JSON file. You can preview the NYC JSON structure here: https://data.cityofnewyork.us/api/views/ia2d-e54m/rows.json?accessType=DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus**\n",
    "\n",
    "Let's turn the list of lists in which NYC has stored the data into a dataframe. Because the data has been stored as a list of lists, this is a one-step process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "We've lost our column headers! NYC has stored them in a separate location in the JSON. How can we recover them? Hint: take a look at the full JSON here: https://data.cityofnewyork.us/api/views/ia2d-e54m/rows.json?accessType=DOWNLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. SQLite**\n",
    "\n",
    "Once you start working with data distributed across multiple tables, or working with data that is starting to exceed easy usability in a format like CSV, consider implementing Python with a simple SQLite database to push and pull data. The maintainers of SQLite3 claim that it will operate on large data files (see https://www.sqlite.org/limits.html), up to 140 TB of file size for a database--larger than most systems can manipulate in memory, at any rate. So for most purposes, you can nicely store your data in such a database and access it when needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Numpy Array**\n",
    "\n",
    "The numpy module provides a quick way to save complex array-like objects (lists, lists of lists, dictionaries, lists of dictionaries, dictionaries of lists) to a binary-file type .npy file. This can be a quick and easy way (among many others, including saving such objects in a .txt or as a .py file) to save and return to a data container without having to reconstruct it every time you work on a workflow. See also below for how to save the .npy file in the first place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing\n",
    "\n",
    "We've just discussed how to load and parse stored data. Let's look at the opposite side of the coin -- storing data from Python into a saved file.\n",
    "\n",
    "**1. JSON**\n",
    "\n",
    "Let's say we've transformed a data file by adding some new data, or cleaning it. We now have a cleaned data set that we can queue up for analysis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Numpy**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. SQLite**\n",
    "\n",
    "This operation will be straightforward from the concepts outlined above, with the added step that we need to create our database if it does not exist, and our table within the database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. CSV with Pandas**\n",
    "\n",
    "Pandas also has a very quick way to move a dataframe into a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "\n",
    "Finally, we can perform syntax cleaning operations in Python just as easily as in a program like Excel. In fact, if you are working with very large data ( > 1.2 million rows) you will simply not be able to use Excel or other spreadsheet programs to manipulate data and must depend on other methods.\n",
    "\n",
    "Here, let's take a look at a very simple data set (2 columns, 66 rows) consisting of the Significant Noncompliance List published by NYC Open Data: https://data.cityofnewyork.us/Environment/Significant-Noncompliance-List/xnje-s6zf\n",
    "\n",
    "If you preview the data, you'll notice that a date span has been combined into a single row cell, that there is some inconsistencies in how months are listed, and it implements a kind of \"footnote\" method, with starred/crossed establishment names referring to the following notes:\n",
    "\n",
    " - **These establishments are on the list for late reporting only.*\n",
    " - *†These establishments are Non-Significant Industrial Users.*\n",
    "\n",
    "How would we perform the following data cleaning operations:\n",
    "\n",
    " 1) Split the \"period\" column into two columns, a start_date and end_date column?\n",
    " 2) Standarize all spelling of months\n",
    " 3) Create a new columns conveying the information in the \"footnote\" marks?\n",
    " 4) Remove commas, periods, etc. from the establishment names\n",
    " 5) Ensure there is no whitespace or newlines on the ends of entries\n",
    " 6) Output the cleaned data as a CSV in the /cleaned_data folder\n",
    "\n",
    "Try this challenge using whatever data container you wish. You may want to consider a very simple one: we can turn the CSV into a list of lists by reading in the file, splitting it on the \", combination, and working with each value separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(directory + '/raw_data/nyc-noncompliancelist.csv') as dfile:\n",
    "    rows = dfile.readlines()\n",
    "\n",
    "for row in rows[1:]:            # Taking the rows as a list, but skipping the first header row\n",
    "    print(row.split('\",'))      # Splitting on the delimiter which we notice the escaped \" before it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Challenge: BPL Branches Data\n",
    " \n",
    "Time permitting, let's put our full list of skills together and see if we can extract a simplified dataset from this Brooklyn Public Library <a href=\"https://data.cityofnewyork.us/Recreation/BPL-Branches/xmzf-uf2w\">listing</a> of library branches and hours. Using the JSON formatted file available at https://www.bklynlibrary.org/locations/json, extract the name of each branch, its address, and a third column listing the total number of hours per week that the branch is open.  \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
